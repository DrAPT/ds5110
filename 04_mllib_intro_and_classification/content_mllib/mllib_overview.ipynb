{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"uva_seal.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning with MLlib\n",
    "## *Introduction and Feature Extraction*\n",
    "\n",
    "### University of Virginia\n",
    "### DS 5110: Big Data Systems\n",
    "### Last Updated: February 15, 2022\n",
    "\n",
    "---  \n",
    "\n",
    "### SOURCES \n",
    "\n",
    "1. Learning Spark\n",
    "2. Spark Documentation  \n",
    "\thttps://spark.apache.org/docs/latest/mllib-data-types.html  \n",
    "\thttp://spark.apache.org/docs/1.2.0/mllib-feature-extraction.html\n",
    "\n",
    "### OBJECTIVES\n",
    "1. Introduction to the machine learning library\n",
    "2. Introduction to MLlib data types\n",
    "3. Discuss Feature Extraction tools in MLLib\n",
    "\n",
    "\n",
    "### CONCEPTS AND FUNCTIONS\n",
    "- pipeline  \n",
    "- supervised and unsupervised learning  \n",
    "- learning tasks: classification, regression, clustering, dimensionality reduction  \n",
    "- training set, testing set  \n",
    "- feature extraction  \n",
    "\n",
    "- MLlib data types:  \n",
    "  - LabeledPoint  \n",
    "  - sparse vector, dense vector  \n",
    "  - sparse matrix, dense matrix  \n",
    "  - Rating  \n",
    "\n",
    "- Feature Extraction  \n",
    "- TF-IDF  \n",
    "- Word2Vec  \n",
    "- Cosine Similarity  \n",
    "\n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supervised Learning vs Unsupervised Learning**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In *supervised learning* tasks, each observation has a label or ground truth indicating the correct answer.  \n",
    "Unsupervised learning tasks do NOT have this label. Most data in the wild does not have the label.\n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine Learning in Spark**  \n",
    "Spark MLlib is the library for machine learning.  There are two interfaces:\n",
    "\n",
    "1) A newer DataFrame-based API which is being actively built out\n",
    "\n",
    "2) An older RDD-based API which is still maintained, but it is not growing  \n",
    "  For supervised learning tasks, the RDD API uses a `LabeledPoint` object to bundle labels with predictors.\n",
    "  \n",
    "  For unsupervised learning tasks, since there is no label, the `LabeledPoint` object is not used.  \n",
    "  Examples of unsupervised learning tasks include clustering methods like k-means.\n",
    "\n",
    "Some functionality is only available in the RDD-based API.  \n",
    "We will discuss both APIs in this course. \n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipelines**\n",
    "\n",
    "MLlib includes a pipeline API useful for building ML pipelines, similar to scikit-learn in Python.  It is HIGHLY recommended that you use pipelines.  They encapsulate the process, reducing the chance of errors, and making the scoring process simple.  More on pipelines later.\n",
    "\n",
    "Next, we jump right in, building a classifier and making predictions. You might not yet know about objects like `LabeledPoint`, but this should be fun and motivating!\n",
    "\n",
    "**Game Plan**\n",
    "\n",
    "We will begin with the RDD interface, and then transition to the DataFrame interface.  \n",
    "The remainder of this notebook uses the RDD interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LogReg Classifier to Predict Spam vs Not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT MODULES\n",
    "import os\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"mllib_classifier\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://udc-ba27-18:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>mllib_classifier</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd59c512e50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in spam and ham (not spam) data\n",
    "spam = sc.textFile(\"spam.txt\")\n",
    "ham = sc.textFile(\"ham.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dear sir, I am a Prince in a far kingdom you have not heard of.  I want to send you money via wire transfer so please ...'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dear sir, I am a Prince in a far kingdom you have not heard of.  I want to send you money via wire transfer so please ...',\n",
       " 'Get Viagra real cheap!  Send money right away to ...',\n",
       " 'Oh my gosh you can be really strong too with these drugs found in the rainforest. Get them cheap right now ...',\n",
       " 'YOUR COMPUTER HAS BEEN INFECTED!  YOU MUST RESET YOUR PASSWORD.  Reply to this email with your password and SSN ...',\n",
       " 'THIS IS NOT A SCAM!  Send money and get access to awesome stuff really cheap and never have to ...']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note you wouldn't collect to driver if RDD was massive\n",
    "spam.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dear Spark Learner, Thanks so much for attending the Spark Summit 2014!  Check out videos of talks from the summit at ...',\n",
       " 'Hi Mom, Apologies for being late about emailing and forgetting to send you the package.  I hope you and bro have been ...',\n",
       " 'Wow, hey Fred, just heard about the Spark petabyte sort.  I think we need to take time to try it out immediately ...',\n",
       " 'Hi Spark user list, This is my first question to this list, so thanks in advance for your help!  I tried running ...',\n",
       " \"Thanks Tom for your email.  I need to refer you to Alice for this one.  I haven't yet figured out that part either ...\",\n",
       " 'Good job yesterday!  I was attending your talk, and really enjoyed it.  I want to try out GraphX ...',\n",
       " 'Summit demo got whoops from audience!  Had to let you know. --Joe']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a Term Frequency object using the hashing trick\n",
    "tf = HashingTF(numFeatures = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the datasets, parsing on spaces\n",
    "spamFeatures = spam.map(lambda email: tf.transform(email.split(\" \")))\n",
    "normalFeatures = ham.map(lambda email: tf.transform(email.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(10000, {0: 1.0, 365: 1.0, 455: 1.0, 509: 1.0, 1320: 1.0, 1363: 2.0, 1583: 1.0, 2321: 2.0, 2403: 1.0, 3289: 2.0, 3342: 1.0, 4995: 1.0, 5336: 1.0, 5706: 1.0, 5831: 1.0, 6052: 1.0, 6300: 1.0, 6582: 1.0, 6744: 1.0, 8971: 1.0, 8977: 1.0, 9232: 1.0, 9604: 1.0, 9646: 1.0, 9878: 1.0}),\n",
       " SparseVector(10000, {0: 1.0, 365: 1.0, 940: 1.0, 2220: 1.0, 3122: 1.0, 4460: 1.0, 4671: 1.0, 5336: 1.0, 5849: 1.0, 8479: 1.0, 9604: 1.0})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spamFeatures.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalFeatures.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Examining the *HashingTF* example**\n",
    "\n",
    "Recall we selected vocab size 10K, so sparse vectors will have this length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,[4511,4946,9026],[1.0,1.0,2.0])\n",
      "(10000,[4946,9026,9421],[1.0,1.0,1.0])\n"
     ]
    }
   ],
   "source": [
    "doc1 = 'cat the tabby cat'\n",
    "doc2 = 'the siamese cat'\n",
    "\n",
    "# tokenize the docs and hash them to term frequencies\n",
    "print(tf.transform(doc1.split(\" \")))\n",
    "print(tf.transform(doc2.split(\" \")))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each document is represented as a vector.  \n",
    "Only words appearing in the doc are reflected in the sparse vector.  \n",
    "Notice for the common words \"the\" and \"cat\", there are location matches between the document representations.  \n",
    "As 'cat' appears twice in the first doc, this tells us the location of the word 'cat'.   \n",
    "You can work out which locations store the remaining words.  \n",
    "The analyst won't need to apply this mapping in practice, but it is instructive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build LabeledPoint datasets (1=spam, 0=ham)  \n",
    "LabeledPoints package (label, features) for each record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "positiveExamples = spamFeatures.map(lambda features: LabeledPoint(1, features))\n",
    "negativeExamples = normalFeatures.map(lambda features: LabeledPoint(0, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = positiveExamples.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, (10000,[0,365,455,509,1320,1363,1583,2321,2403,3289,3342,4995,5336,5706,5831,6052,6300,6582,6744,8971,8977,9232,9604,9646,9878],[1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = negativeExamples.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(0.0, (10000,[0,1162,2403,2809,3080,3317,4161,4770,5423,5651,5743,5831,6006,6827,6971,7069,7872,9150,9370,9521,9604],[1.0,2.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UnionRDD[7] at union at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build training set; this stacks positive and negative records\n",
    "trainData = positiveExamples.union(negativeExamples)\n",
    "\n",
    "# cache since model training is recursive; o.w. would rebuild DataFrame\n",
    "trainData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train LogReg model using default params\n",
    "model = LogisticRegressionWithSGD.train(trainData, iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push \"not spam\" example through classifier. this is label=0\n",
    "Test = tf.transform(\"I love learning Spark programming\".split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "print(\"Prediction for example: {}\".format(model.predict(Test)))\n",
    "if model.predict(Test)==0:\n",
    "    print(\"CORRECT!\")\n",
    "else:\n",
    "    print(\"INCORRECT!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "\n",
    "Next we define the `MLlib` objects.\n",
    "\n",
    "**LabeledPoint**  \n",
    "Stores feature vector together with label  \n",
    "\n",
    "**Rating**  \n",
    "Rating of product by a user. Used in recommendation, for instance.  \n",
    "\n",
    "**Vector**  \n",
    "Handles dense and sparse. For sparse, only nonzero values and their indices are stored, along w vector length.  \n",
    "Sparse saves on memory and runtime.  \n",
    "\n",
    "**Matrix**  \n",
    "A local matrix has integer-typed row and column indices and double-typed values, stored on a single   machine.  \n",
    "MLlib supports dense matrices, whose entry values are stored in a single double array in column-major order, and sparse matrices, whose non-zero entry values are stored in the Compressed Sparse Column (CSC) format in column-major order.  \n",
    "\n",
    "**Distributed matrix**  \n",
    "A distributed matrix has long-typed row and column indices and double-typed values  \n",
    "\n",
    "**Row matrix**  \n",
    "A RowMatrix is a row-oriented distributed matrix without meaningful row indices  \n",
    "\n",
    "**CoordinateMatrix**  \n",
    "CoordinateMatrix is a distributed matrix backed by an RDD of its entries  \n",
    "A CoordinateMatrix should be used only when both dimensions of the matrix are huge and the matrix is very sparse.\n",
    "\n",
    "Take a look at this wiki to learn about row- versus column-major order.  It is super important to know how the data is saved.  Could you imagine what would happen to results if this were mixed up?\n",
    "\n",
    "https://en.wikipedia.org/wiki/Row-_and_column-major_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "*mllib.feature*  \n",
    "contains classes for common feature transformations:  \n",
    "-  Term Frequency-Inverse Document Frequency (TF-IDF)  \n",
    "Produces feature vectors from text documents\n",
    "\n",
    "There are two algorithms that compute TF-IDF:  \n",
    "\n",
    "**1. HashingTF**  \n",
    "\tComputes term frequency vector from document  \n",
    "\tCan process one document or an RDD of documents  \n",
    "\tEach document needs to be an interable sequence (a list in Python)  \n",
    "\n",
    "To reduce the chance of collision, we can increase the target feature dimension, i.e., the  \n",
    "\t number of buckets of the hash table. The default feature dimension is 1,048,576  \n",
    "\n",
    "**2. IDF**  \n",
    "\tComputes inverse document frequency  \n",
    "\tTerms that appear in high fraction of the docs are not as valuable  \n",
    "\tIDF will downweight such terms  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a good example of Feature Extraction:  \n",
    "http://spark.apache.org/docs/1.2.0/mllib-feature-extraction.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec**  \n",
    "Computes distributed vector representation of words.  \n",
    "Similar words are close in the vector space  \n",
    "Useful in many NLP applications:  \n",
    "named entity recognition, disambiguation, parsing, tagging and machine translation.  \n",
    "\n",
    "The algorithm uses a neural network and some interesting concepts like the *hierarchical softmax*.  I encourage you to learn more if you have the time and interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Word2VecModel to some text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import Word2Vec\n",
    "\n",
    "inp = sc.textFile(\"fed_rates_article.txt\").map(lambda row: row.split(\" \"))\n",
    "topk = 5\n",
    "print('First {} records:'.format(topk))\n",
    "first_five = inp.take(topk)\n",
    "for i in range(topk):\n",
    "    print(first_five[i])\n",
    "print(\"-----------------\")\n",
    "                        \n",
    "word2vec = Word2Vec() # construct Word2Vec object\n",
    "model = word2vec.fit(inp) # train Word2Vec on the dasta\n",
    "\n",
    "# apply Word2Vec to find synonyms by representing words as vectors\n",
    "synonyms = model.findSynonyms('rate', 20)\n",
    "\n",
    "for word, cosine_distance in synonyms:\n",
    "    print(\"{}: {}\".format(word, cosine_distance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StandardScaler**   \n",
    "\n",
    "Standardization can improve the convergence rate during the optimization process, and it also prevents against features with very large variances exerting an overly large influence during model   training.  \n",
    "\n",
    "For each feature,  \n",
    "1. Scales to unit variance  \n",
    "2. Centers to mean zero  \n",
    "Useful or even essential for some models  \n",
    "\n",
    "`K-means` works in Euclidean space, so all features should be on same scale  \n",
    "\n",
    "Tree models do not need this\n",
    "\n",
    "Use this in a *Pipeline* so the statistics can be applied to datasets for scoring later. You would NOT compute means and standard deviations on the scoring set to standardize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaler  \n",
    "Load dataset in libsvm format, standardize the features so that the new features have unit variance and/or zero mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MLUtils.loadLibSVMFile(sc, \"sample_libsvm_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract labels and features; stored as RDDs\n",
    "label = data.map(lambda x: x.label)\n",
    "features = data.map(lambda x: x.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1 = StandardScaler().fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1.transform(features).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1 will be unit variance.\n",
    "data1 = label.zip(scaler1.transform(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRY FOR YOURSELF (UNGRADED EXERCISES)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Print the label and features (before scaling) from the first record in *data*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Compute the first 20 synonyms of the word \"economy.\" Then extract and print the cosine distances.  Do the results make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Copy the Ham/Spam classifier code in the cell below.  Then try a different model, leaving the rest of the code unchanged.  Run the code.  Does it get the \"not spam\" example right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
