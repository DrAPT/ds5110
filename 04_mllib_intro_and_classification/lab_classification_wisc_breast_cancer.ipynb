{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### University of Virginia\n",
    "### DS 5110: Big Data Systems\n",
    "\n",
    "### Classification of Wisconsin Breast Cancer Database\n",
    "### Last updated: February 11, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions** \n",
    "\n",
    "In this project, you will work with the Wisconsin Breast Cancer dataset.  You will train a logistic regression model to predict the diagnosis.  First, you will work through this example, **filling in the missing cells.** Then you will complete followup experiments below.\n",
    "\n",
    "**Total Possible Points: 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param init \n",
    "infile = 'wisc_breast_cancer_w_fields.csv'\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Wisc BRCA\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data into dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 PT)** Combine fields *f1*, *f2*, *f3* into a single *features* column using `VectorAssembler`  \n",
    "Name the resulting dataframe *transformed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the *diagnosis* and *features* fields for modeling.  \n",
    "We will do the remaining steps with RDDs, so we convert to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRdd = transformed.select(\"diagnosis\", \"features\").rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at some data\n",
    "dataRdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map label to binary values, then convert to LabeledPoint\n",
    "lp = dataRdd.map(lambda row:(1 if row[0]=='M' else 0, Vectors.dense(row[1])))    \\\n",
    "                    .map(lambda row: LabeledPoint(row[0], row[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 PT)** Split data approximately into training (60%) and test (40%) using `seed=314`  \n",
    "The RDDs that are output from the splitting should be named *training*, *test*, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 PT)** Train model `LogisticRegressionWithLBFGS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 PT)**  Evaluate the model by computing the accuracy on the **test data**. Print the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "\n",
    "Next, you will conduct the experiments described below. \n",
    "\n",
    "2.  Three features were used in the original model.  **Build the model using all features.**\n",
    "Before training the model, apply scaling to the features using the StandardScaler\n",
    "transformer.  Then train the model and compute and show the accuracy and confusion matrix, **measured on the test set.**\n",
    "\n",
    "**Hint**: While the data is in a dataframe, this might be helpful:\n",
    "```\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "```\n",
    "\n",
    "3. Repeat step (2), including an intercept\n",
    "4. Repeat step (2), using randomSplit([0.7, 0.3]) but NO intercept\n",
    "\n",
    "**Total Possible Points: 10**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTIONS**  \n",
    " For parts 2-4, compute and show for the test set: (1) accuracy (2) confusion matrix.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter solution for Part 2 (2 POINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter solution for Part 3  (2 POINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter solution for Part 4 (2 POINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "DS5110 Spark 3.3",
   "language": "python",
   "name": "ds5110_spark3.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
