{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"uva_seal.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Spark on a Cluster\n",
    "\n",
    "### University of Virginia\n",
    "### DS 5110: Big Data Systems\n",
    "### Last Updated: Feb 16, 2021\n",
    "\n",
    "---  \n",
    "\n",
    "### SOURCES \n",
    "\n",
    "1. Learning Spark, Chapter 7: Running on a Cluster\n",
    "\n",
    "### OBJECTIVES\n",
    "- Learn how to run distributed Spark\n",
    "- Learn about some of the common deployment environments\n",
    "\n",
    "\n",
    "### CONCEPTS AND FUNCTIONS\n",
    "- Cluster manager (Hadoop YARN, Apache Mesos, Standalone)\n",
    "- Driver and worker/executor\n",
    "- Spark application\n",
    "- Directed acyclic graph (DAG)\n",
    "- Build tool\n",
    "- Assembly JAR\n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Architecture\n",
    "\n",
    "One benefit of Spark is the ability to scale computation by adding more machines and running in cluster mode\n",
    "\n",
    "*Driver* is in charge of coordinating the workers\n",
    "\n",
    "The *workers* / *executors* receive code and data and do the processing, sending results back to driver.\n",
    "\n",
    "Driver + Workers = Spark application\n",
    "\n",
    "### Driver\n",
    "\n",
    "`main()` method of program runs on driver\n",
    "\n",
    "Converts program into tasks\n",
    "\n",
    "Converts into logical *directed acyclic graph* (DAG) of operations\n",
    "\n",
    "Coordinates scheduling of tasks on executors (like a manager)\n",
    "\n",
    "### Executors\n",
    "\n",
    "Run the individual tasks\n",
    "\n",
    "Launch at start of application and run for lifetime of app\n",
    "\n",
    "Provide in-memory (RAM) storage for RDDs\n",
    "\n",
    "### Cluster Manager\n",
    "\n",
    "External service where the Spark application runs.  \n",
    "\n",
    "Spark is packaged with the Standalone cluster manager.\n",
    "\n",
    "Manages the resources between Spark applications.  \n",
    "Can manage queues if there is more demand than resources for executors.\n",
    " \n",
    "### Launching a Program\n",
    "`spark-submit` is called to launch a Spark app\n",
    "\n",
    "**Run in local mode using single core**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "`$ bin\\spark-submit --master local python_scripts\\textAnalysis1.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run in local mode using 4 cores**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`$ bin\\spark-submit --master local[4] python_scripts\\textAnalysis1.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run in local mode using all cores**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "`$ bin\\spark-submit --master local[*] python_scripts\\textAnalysis1.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run on Spark Standalone cluster at default port**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`$ bin\\spark-submit --master spark://host:7077 python_scripts\\textAnalysis1.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run on Spark Standalone cluster at default port, specifying memory to allocate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "`$ bin\\spark-submit --master spark://host:7077 –-executor_memory 10g \tpython_scripts\\textAnalysis1.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generic Form to run Spark App**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "`$ bin\\spark-submit [options] <app jar | python file> [app options]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can include various flags in the short or long format `-shortflag` and \n",
    "`--longflag` respectively  \n",
    "\n",
    "The flags control scheduling information and dependencies such as libraries and files\n",
    "\n",
    "For a list of all flags issue:  \n",
    "`bin\\spark-submit --help`\n",
    "\n",
    "\n",
    "### Spark Web UI\n",
    "\n",
    "Spark comes with a built-in Web UI.  There are several tabs such as `Jobs` and `Stages` which provide details about the running application.  Useful information such as resources used at each stage of the computation is available here.\n",
    "\n",
    "When running jobs locally (*local mode*), you should be able to view the UI at this URL:  \n",
    "http://localhost:4040/jobs/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"spark_app_mgr.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packaging Code and Dependencies  \n",
    "\n",
    "**Python**  \n",
    "PySpark uses Python on worker machines, so can use `pip`  \n",
    "Can also submit libraries using the `--py-files` argument to `spark-submit`  \n",
    "\n",
    "**Java and Scala**   \n",
    "Users will submit individual JAR files using `--jars`  \n",
    "For a large set of dependencies, it is better to use a build tool (`sbt` or `Maven`) to package all dependencies into one JAR called the *assembly JAR*.\n",
    "\n",
    "`Maven` produces a pom.xml file containing a build definition.\n",
    "\n",
    "A *Project Object Model* or *POM* is the fundamental unit of work in `Maven`. It is an XML file that contains information about the project and configuration details used by `Maven` to build the project. It contains default values for most projects.\n",
    "\n",
    "https://maven.apache.org/guides/introduction/introduction-to-the-pom.html\n",
    "\n",
    "Packaging a spark application built with `Maven` is straightforward.    \n",
    "\n",
    "**Run on Spark Standalone Cluster at Default Port**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "---  \n",
    "```\n",
    "$ mvn package          # create the assembly JAR\n",
    "\n",
    "# The assembly JAR will be placed in the target directory\n",
    "$ bin\\spark-submit --master local … target\\name_of_assembly.jar\n",
    "```\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop YARN\n",
    "\n",
    "**Y**et **A**nother **R**esource **N**egotiator \n",
    "\n",
    "`YARN` is a cluster manager introduced in `Hadoop 2.0`  \n",
    "It does the following:\n",
    "- allocates system resources to various applications running in a `Hadoop` cluster.  \n",
    "- schedules tasks to be executed on different cluster nodes  \n",
    "\n",
    "`YARN` is installed on same nodes as `HDFS`, making it quicker to access data.  \n",
    "\n",
    "To use `YARN` in Spark, set an environment variable that points to the `Hadoop` config directory, then submit jobs to a special master URL with `spark-submit`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "---  \n",
    "```\n",
    "export HADOOP_CONF_DIR=\"...\"\n",
    "spark-submit --master yarn appname\n",
    "```\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default `YARN` uses 2 executors, so you will likely need to change the setting with flag:  \n",
    "`--num-executors`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"yarn.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Resource Manager` accepts jobs from users, schedules them and allocates resources  \n",
    "\n",
    "`Node Manager` monitors the node and provides reporting\n",
    "\n",
    "`Application Master` is created for each application to negotiate for resources and work with `NodeManager` to execute and monitor tasks.\n",
    "\n",
    "`Containers` are controlled by the `NodeManager` and assigned system resources.\n",
    "\n",
    "\n",
    "### Amazon EC2 (elastic cloud compute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark has built-in script to launch clusters on EC2: `spark-ec2`\n",
    "\n",
    "Will need Amazon Web Services (AWS) account  \n",
    "Export the *access key ID* and *secret access key*    \n",
    "By default, launching the cluster produces one master and one worker  \n",
    "Storage: Spark EC2 clusters include two installations of `HDFS`  \n",
    "See Learning Spark p 136 for details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRY FOR YOURSELF (UNGRADED EXERCISES)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AWS**  \n",
    "If you are interested in learning about the AWS products - which are comprehensive across the cloud computing space - there is an AWS Free Tier.\n",
    "\n",
    "Please refer here for details:  \n",
    "https://aws.amazon.com/free/?all-free-tier.sort-by=item.additionalFields.SortRank&all-free-tier.sort-order=asc\n",
    "\n",
    "From AWS:  \n",
    "*Explore more than 60 products and start building on AWS using the free tier. Three different types of free offers are available depending on the product used.*\n",
    "\n",
    "- Always free\n",
    "- 12 months free\n",
    "- Trials\n",
    "\n",
    "There will be a separate course document outlining some optional work in AWS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Amazon Elastic MapReduce (EMR)**\n",
    "\n",
    "`Amazon EMR` provides a managed `Hadoop` framework to process vast amounts of data using AWS for parallel, distributed, elastic execution of data processes and tasks. `EMR` leverages `S3`, which is their elastic, highly reliable cloud storage product (covered later in the course). \n",
    "  \n",
    "Here is a very short overview (1 min) of EMR:  \n",
    "https://www.youtube.com/watch?v=AM8WZb2Xj2g\n",
    "\n",
    "As a separate assignment, you will watch a video providing a deeper dive into `Amazon EMR`."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "DS 5559",
   "language": "python",
   "name": "ds5559"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
