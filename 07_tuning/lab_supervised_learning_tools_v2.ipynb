{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### University of Virginia \n",
    "### DS 5110: Big Data Systems\n",
    "### Assignment: Tools for Supervised Learning\n",
    "### Last Updated: April 7, 2022\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INSTRUCTIONS**  \n",
    "In this assignment, you will code functions to support supervised learning tasks.  The outline is provided below.  The value *None* is used as a placeholder. For random sampling use seed=314 throughout. Each task should be treated independently.\n",
    "\n",
    "**TOTAL POINTS: 10**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE TO GRADER**  \n",
    "Results might vary slightly due to factors like number of cores, whether RDD or DataFrame API is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"data preprocessing\") \\\n",
    "    .config(\"spark.executor.memory\", '8g') \\\n",
    "    .config('spark.executor.cores', '4') \\\n",
    "    .config('spark.cores.max', '4') \\\n",
    "    .config(\"spark.driver.memory\",'8g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# update to match your path\n",
    "directory_path = '/sfs/qumulo/qhome/apt4c/ds5110/assignments/M7_8_supervised_tools/'\n",
    "full_path_to_file = os.path.join(directory_path, 'breast_cancer_wisconsin.csv')\n",
    "path_to_data = os.path.join(full_path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class = 2 for benign (negative class, 4 for malignant (positive class)\n",
    "target = 'class'\n",
    "positive_label = 4\n",
    "negative_label = 2\n",
    "\n",
    "SEED = 314"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READ IN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "brca = spark.read.csv(path_to_data, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- clump_thickness: integer (nullable = true)\n",
      " |-- uniformity_cell_size: integer (nullable = true)\n",
      " |-- uniformity_cell_shape: integer (nullable = true)\n",
      " |-- marginal_adhesion: integer (nullable = true)\n",
      " |-- single_epithelial_cell_size: integer (nullable = true)\n",
      " |-- bare_nuclei: string (nullable = true)\n",
      " |-- bland_chromatin: integer (nullable = true)\n",
      " |-- normal_nucleoli: integer (nullable = true)\n",
      " |-- mitoses: integer (nullable = true)\n",
      " |-- class: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "brca.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "699"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brca.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|class|count|\n",
      "+-----+-----+\n",
      "|    4|  241|\n",
      "|    2|  458|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute distribution of target variable\n",
    "brca.groupBy(target).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1:  Cross Validate a Logistic Regression Model\n",
    "i) (**4 PTS**) This task has the following requirements:\n",
    "- import necessary modules\n",
    "- use these features as predictors: `clump_thickness`,`uniformity_cell_size`,`uniformity_cell_shape`,`marginal_adhesion`  \n",
    "- `class` is response variable. apply recoding as needed. hint: save as new variable.\n",
    "- use 3 folds in the cross validator object\n",
    "- use BinaryClassificationEvaluator\n",
    "- logistic regression model with `maxIter`=10  \n",
    "- tuning grid with `regParam` values of 0.1 and 0.01\n",
    "- finally, print the average metrics based on each `regParam` value. the attribute `avgMetrics` in the cv model will hold these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2:  Balancing a DataFrame with Downsampling  \n",
    "i) (**2 PTS**) Write a function to implement downsampling.  Enter code into the cell containing the `downsample` function.  \n",
    "\n",
    "INPUTS  \n",
    "* df               - Spark dataframe  \n",
    "* target           - string, target variable  \n",
    "* positive_label   - integer, value of positive label  \n",
    "* negative_label   - integer, value of negative label  \n",
    "\n",
    "OUTPUT  \n",
    "balanced spark dataframe  \n",
    "\n",
    "Downsampling = sample from larger class to match smaller class  \n",
    "\n",
    "**Example:**  \n",
    "\n",
    "INITIAL STATE  \n",
    "Smaller class has 100 records  \n",
    "Larger class size has 400 records\n",
    "\n",
    "ACTION  \n",
    "Sample 100 records from larger class, without replacement  \n",
    "Retain all records from smaller class\n",
    "\n",
    "END STATE    \n",
    "This produces a balanced dataset containing 100 records from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) **(1 PT)** Print the target distribution from this balanced dataset, to show the label counts nearly match.\n",
    "\n",
    "#### IMPORTANT NOTE:\n",
    "Sampling won't produce the exact fraction you request. In order to sample efficiently, Spark uses Bernouilli Sampling. \n",
    "Each row is assigned a probability of being included. If you request a 10% sample, each row individually has a 10% chance of being included but this does not guarantee an exact 10% sample   \n",
    "(it should be close, however)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3:  Univariate AUC Measurement  \n",
    "\n",
    "In this exercise, you will measure (in a particular sense) the individual predictive power of the following variables:  \n",
    "* clump_thickness\n",
    "* uniformity_cell_size\n",
    "* uniformity_cell_shape\n",
    "* marginal_adhesion\n",
    "* single_epithelial_cell_size\n",
    "\n",
    "**(3 PTS)** Define a function called `compute_univariate_aucs`  \n",
    "The function needs to do the following:\n",
    "\n",
    "* Split the dataset into training and testing sets (*60% / 40%*, respectively)  \n",
    "* For each variable v:  \n",
    "    * train a logistic regression classifier with intercept, including variable *v* as predictor\n",
    "    * classify each record in the test set  \n",
    "    * measure the area under the ROC curve  \n",
    "* Return a dataframe containing each variable, its model weight (coefficient), and its Univariate AUC, sorted by Univariate AUC in descending order.\n",
    "\n",
    "INPUTS  \n",
    "* df, Spark dataframe \n",
    "* target variable as string\n",
    "* training_fraction  \n",
    "* max_iterations  \n",
    "* seed  \n",
    "\n",
    "OUTPUTS  \n",
    "dataframe containing three columns: variable name, weight, AUROC  \n",
    "\n",
    "#### IMPORTANT NOTES:   \n",
    "1) If you use the RDD API, LabeledPoint requires that positive label = 1, negative label = 0  \n",
    "2) Do NOT use the downsampling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the `compute_univariate_aucs` and print the results from the dataframe.  Remember not to downsample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
